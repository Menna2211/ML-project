{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdLZw4eoc5M0"
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKY65oZ7eMzR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "419f92ee-e05a-4baa-8e15-1b7b08aeee55"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7f7dd5d6-e0e4-46e5-b800-2b463f0d39e5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7f7dd5d6-e0e4-46e5-b800-2b463f0d39e5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving pg11.txt to pg11.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmNUPDQTeJK2"
      },
      "source": [
        "filename = \"pg11.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqdkbS72eXdp"
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPf3A5evebSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e3f2497-adfc-4eec-ff9f-b84220cc91ae"
      },
      "source": [
        "chars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " '@',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '\\ufeff']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxtnWdbcegpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5e88b64-cd80-4fe9-cecf-4c812052895e"
      },
      "source": [
        "char_to_int"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " '#': 4,\n",
              " '$': 5,\n",
              " '%': 6,\n",
              " \"'\": 7,\n",
              " '(': 8,\n",
              " ')': 9,\n",
              " '*': 10,\n",
              " ',': 11,\n",
              " '-': 12,\n",
              " '.': 13,\n",
              " '/': 14,\n",
              " '0': 15,\n",
              " '1': 16,\n",
              " '2': 17,\n",
              " '3': 18,\n",
              " '4': 19,\n",
              " '5': 20,\n",
              " '6': 21,\n",
              " '7': 22,\n",
              " '8': 23,\n",
              " '9': 24,\n",
              " ':': 25,\n",
              " ';': 26,\n",
              " '?': 27,\n",
              " '@': 28,\n",
              " '[': 29,\n",
              " ']': 30,\n",
              " '_': 31,\n",
              " 'a': 32,\n",
              " 'b': 33,\n",
              " 'c': 34,\n",
              " 'd': 35,\n",
              " 'e': 36,\n",
              " 'f': 37,\n",
              " 'g': 38,\n",
              " 'h': 39,\n",
              " 'i': 40,\n",
              " 'j': 41,\n",
              " 'k': 42,\n",
              " 'l': 43,\n",
              " 'm': 44,\n",
              " 'n': 45,\n",
              " 'o': 46,\n",
              " 'p': 47,\n",
              " 'q': 48,\n",
              " 'r': 49,\n",
              " 's': 50,\n",
              " 't': 51,\n",
              " 'u': 52,\n",
              " 'v': 53,\n",
              " 'w': 54,\n",
              " 'x': 55,\n",
              " 'y': 56,\n",
              " 'z': 57,\n",
              " '\\ufeff': 58}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfwzRyPrelPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db3b8c0e-9d65-405e-fd08-cf1bdcec5e73"
      },
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  163781\n",
            "Total Vocab:  59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEfRrajueyYx"
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E23bIwfKe7jN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7cd0395-9ef0-4bc2-efde-345ac9320b13"
      },
      "source": [
        "n_patterns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "163681"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GZNoLFKe-j9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e002b1f5-0439-4acf-95b7-fe8589259e4a"
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1277/1279 [============================>.] - ETA: 0s - loss: 2.9734\n",
            "Epoch 1: loss improved from inf to 2.97336, saving model to weights-improvement-01-2.9734.hdf5\n",
            "1279/1279 [==============================] - 25s 13ms/step - loss: 2.9734\n",
            "Epoch 2/20\n",
            "1278/1279 [============================>.] - ETA: 0s - loss: 2.7993\n",
            "Epoch 2: loss improved from 2.97336 to 2.79919, saving model to weights-improvement-02-2.7992.hdf5\n",
            "1279/1279 [==============================] - 18s 14ms/step - loss: 2.7992\n",
            "Epoch 3/20\n",
            "1276/1279 [============================>.] - ETA: 0s - loss: 2.7246\n",
            "Epoch 3: loss improved from 2.79919 to 2.72444, saving model to weights-improvement-03-2.7244.hdf5\n",
            "1279/1279 [==============================] - 17s 14ms/step - loss: 2.7244\n",
            "Epoch 4/20\n",
            "1279/1279 [==============================] - ETA: 0s - loss: 2.6590\n",
            "Epoch 4: loss improved from 2.72444 to 2.65896, saving model to weights-improvement-04-2.6590.hdf5\n",
            "1279/1279 [==============================] - 18s 14ms/step - loss: 2.6590\n",
            "Epoch 5/20\n",
            "1276/1279 [============================>.] - ETA: 0s - loss: 2.6070\n",
            "Epoch 5: loss improved from 2.65896 to 2.60681, saving model to weights-improvement-05-2.6068.hdf5\n",
            "1279/1279 [==============================] - 18s 14ms/step - loss: 2.6068\n",
            "Epoch 6/20\n",
            "1277/1279 [============================>.] - ETA: 0s - loss: 2.5573\n",
            "Epoch 6: loss improved from 2.60681 to 2.55736, saving model to weights-improvement-06-2.5574.hdf5\n",
            "1279/1279 [==============================] - 18s 14ms/step - loss: 2.5574\n",
            "Epoch 7/20\n",
            "1279/1279 [==============================] - ETA: 0s - loss: 2.5080\n",
            "Epoch 7: loss improved from 2.55736 to 2.50796, saving model to weights-improvement-07-2.5080.hdf5\n",
            "1279/1279 [==============================] - 17s 14ms/step - loss: 2.5080\n",
            "Epoch 8/20\n",
            "1277/1279 [============================>.] - ETA: 0s - loss: 2.4637\n",
            "Epoch 8: loss improved from 2.50796 to 2.46386, saving model to weights-improvement-08-2.4639.hdf5\n",
            "1279/1279 [==============================] - 17s 14ms/step - loss: 2.4639\n",
            "Epoch 9/20\n",
            "1276/1279 [============================>.] - ETA: 0s - loss: 2.4206\n",
            "Epoch 9: loss improved from 2.46386 to 2.42059, saving model to weights-improvement-09-2.4206.hdf5\n",
            "1279/1279 [==============================] - 18s 14ms/step - loss: 2.4206\n",
            "Epoch 10/20\n",
            "1276/1279 [============================>.] - ETA: 0s - loss: 2.3832\n",
            "Epoch 10: loss improved from 2.42059 to 2.38309, saving model to weights-improvement-10-2.3831.hdf5\n",
            "1279/1279 [==============================] - 17s 14ms/step - loss: 2.3831\n",
            "Epoch 11/20\n",
            "1278/1279 [============================>.] - ETA: 0s - loss: 2.3451\n",
            "Epoch 11: loss improved from 2.38309 to 2.34517, saving model to weights-improvement-11-2.3452.hdf5\n",
            "1279/1279 [==============================] - 18s 14ms/step - loss: 2.3452\n",
            "Epoch 12/20\n",
            "1278/1279 [============================>.] - ETA: 0s - loss: 2.3074\n",
            "Epoch 12: loss improved from 2.34517 to 2.30732, saving model to weights-improvement-12-2.3073.hdf5\n",
            "1279/1279 [==============================] - 18s 14ms/step - loss: 2.3073\n",
            "Epoch 13/20\n",
            "1279/1279 [==============================] - ETA: 0s - loss: 2.2745\n",
            "Epoch 13: loss improved from 2.30732 to 2.27451, saving model to weights-improvement-13-2.2745.hdf5\n",
            "1279/1279 [==============================] - 17s 14ms/step - loss: 2.2745\n",
            "Epoch 14/20\n",
            "1278/1279 [============================>.] - ETA: 0s - loss: 2.2442\n",
            "Epoch 14: loss improved from 2.27451 to 2.24429, saving model to weights-improvement-14-2.2443.hdf5\n",
            "1279/1279 [==============================] - 18s 14ms/step - loss: 2.2443\n",
            "Epoch 15/20\n",
            "1277/1279 [============================>.] - ETA: 0s - loss: 2.2116\n",
            "Epoch 15: loss improved from 2.24429 to 2.21169, saving model to weights-improvement-15-2.2117.hdf5\n",
            "1279/1279 [==============================] - 17s 14ms/step - loss: 2.2117\n",
            "Epoch 16/20\n",
            "1278/1279 [============================>.] - ETA: 0s - loss: 2.1832\n",
            "Epoch 16: loss improved from 2.21169 to 2.18310, saving model to weights-improvement-16-2.1831.hdf5\n",
            "1279/1279 [==============================] - 18s 14ms/step - loss: 2.1831\n",
            "Epoch 17/20\n",
            "1277/1279 [============================>.] - ETA: 0s - loss: 2.1537\n",
            "Epoch 17: loss improved from 2.18310 to 2.15384, saving model to weights-improvement-17-2.1538.hdf5\n",
            "1279/1279 [==============================] - 17s 14ms/step - loss: 2.1538\n",
            "Epoch 18/20\n",
            "1279/1279 [==============================] - ETA: 0s - loss: 2.1285\n",
            "Epoch 18: loss improved from 2.15384 to 2.12854, saving model to weights-improvement-18-2.1285.hdf5\n",
            "1279/1279 [==============================] - 18s 14ms/step - loss: 2.1285\n",
            "Epoch 19/20\n",
            "1279/1279 [==============================] - ETA: 0s - loss: 2.1008\n",
            "Epoch 19: loss improved from 2.12854 to 2.10084, saving model to weights-improvement-19-2.1008.hdf5\n",
            "1279/1279 [==============================] - 20s 15ms/step - loss: 2.1008\n",
            "Epoch 20/20\n",
            "1279/1279 [==============================] - ETA: 0s - loss: 2.0788\n",
            "Epoch 20: loss improved from 2.10084 to 2.07883, saving model to weights-improvement-20-2.0788.hdf5\n",
            "1279/1279 [==============================] - 17s 14ms/step - loss: 2.0788\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff1835dc8b0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgbawvy1eIwG",
        "outputId": "2483cadb-fa55-4f0d-d632-01622d96b67a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" ris, and paris is the capital of rome, and\n",
            "rome--no, that's all wrong, i'm certain! i must have been \"\n",
            " to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee to tee tooee the rabbit seree oh the couro, \n",
            "'whel you moow to thl ' said the macci hare.\n",
            "\n",
            "'ie iout then the mors,' shi gatter said to herself, \n",
            "'wou don't thenk the douto see thet ' she said to herself, and sei gort and the whrt on the sooee th the thete rabbit, and the whst on the saad the dad not aerer at the woode  and the mart thin she was no the could to the thite was so the whrt on the sand wo the thete  and the whst on the whrl soee a aoorors toice  and the whst on the siie of the soeee of the sar of the care, and she whst oevel aelen the whrl whe har hn the white rabbit  and the marthr oa bitne that she was not an anl the whst woul   the mac oh the cour and the whst on the whil she was a lont of the oare whth the sabdit, and the whst on the same that she was not an anl the whst worl oh the courd, \n",
            "'whel yhu mo sete th thu ' shi gatter said to herself, ''what i mant to the tooe ' shiught alice. 'io sou to te\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gRPGWINfHj8"
      },
      "source": [
        "# Load LSTM network and generate text\n",
        "import sys\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"wonderland.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "# create mapping of unique chars to integers, and a reverse mapping\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print \"Total Characters: \", n_chars\n",
        "print \"Total Vocab: \", n_vocab\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print \"Total Patterns: \", n_patterns\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "# load the network weights\n",
        "filename = \"weights-improvement-19-1.9435.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print (\"Seed:\")\n",
        "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print (\"\\nDone.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I4w04rifXv8"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}